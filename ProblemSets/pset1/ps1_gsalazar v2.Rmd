---
title: "Problem Set 1 - Estimating Auctions"
author: "Gonzalo Salazar"
date: "01/27/2020"
output: html_document
---

```{r setup + packages, include=FALSE}
knitr::opts_chunk$set(echo = T)

rm(list=ls()) # clean all objects

#getwd() # give us the directory in which we are working in

library(MASS)
library(latex2exp)
library(spatstat)
```

## Loading bids and setting working space
```{r working space}

setwd("/Users/gsalazar/Documents/GitHub/empirical-methods/ProblemSets/pset1/")

dt <- read.csv(file = 'bids1.csv', header=FALSE, dec = ".")
names(dt)[1]<-paste("bids")

```

## Question 1
```{r q1 density of bids}

nBids <- length(dt$bids) # number of bids to draw from an estimated density function

ht    <- hist(dt$bids, main = "Histogram of Bids", xlab = "Bids", col = "darkolivegreen2", breaks = 14) # histogram of bids - note the # of breaks was augmented in order to recognize graphically which empirical density fits best

# Fitting a normal distribution
fitN  <- fitdistr(dt$bids, "normal")
dNORM <- dnorm(seq(0,5,length=nBids), fitN$estimate[1], fitN$estimate[2])

# Using a Gaussian Kernel - note that nrd0 uses the Silverman's ROT
dGAUS <- density(dt$bids, bw.nrd0(dt$bids), adjust = 1, 
                kernel = c("gaussian"),
                weights = NULL)
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA  <- density(dt$bids, bw.nrd0(dt$bids), adjust = 1, 
                kernel = c("epanechnikov"),
                weights = NULL)

```


## Question 2
```{r q2 loocv}
# Least-squares cross-validation for bandwidth - LOOCV

FUN   <- function(optBw){
  fhat=Vectorize(function(x) density(dt$bids,from=x,to=x,n=1,bw=optBw)$y)
  fhati=Vectorize(function(i) density(dt$bids[-i], from=dt$bids[i],to=dt$bids[i],n=1,bw=optBw)$y)
  F=fhati(1:length(dt$bids))
  return(integrate(function(x) fhat(x)^2,-Inf,Inf)$value-2*mean(F))
}

vx   <- seq(.05,0.6,by=.01)
vy   <- Vectorize(FUN)(vx) 

plot(vx,vy, col = "darkgreen", lwd = 2)

hOpt <- optimize(FUN,interval=c(.05,6))

hCV  <- hOpt$minimum

dEPA_CV <- density(dt$bids, bw=hCV, adjust = 1, 
                kernel = c("epanechnikov"),
                weights = NULL, n=nBids)

```

## Question 3
```{r q3 density plot}

plot(ht, freq = FALSE, xlim=c(0,5) ,ylim=c(0,.6), col = "darkolivegreen2", xlab = "Bid Values", main = "Histogram of bids + Estimated densities")

lines(seq(0,5,length = nBids),dNORM, col = "darkorchid1", lwd = 3)
lines(dGAUS, col = "deepskyblue", lwd = 4)
lines(dEPA, col = "green4", lty=1, lwd = 3)
lines(dEPA_CV, col = "maroon", lty=6, lwd = 3)

legend(3.5, .55, legend=c("Normal fit","Gaussian (ROT)", "Epanechnikov (ROT)", "Epanechnikov (LOOCV)"),
       col=c("darkorchid1","deepskyblue", "green4", "maroon"), lty = c(1,1,1,6), lwd = c(3,4,3,3), cex=.75)

```

Given the plot above, it seems to be that the empirical Gaussian kernel distribution (with plug-in estimate for the optimal bandwidth) is the best fit for the true distribution. This is because, it captures very well the fall in the bids after the value of 2, and then the increase when these are close to 2.5.

## Question 4

```{r q4 valuations}

nBidders <- 3 # number of bidders

# Estimated density function
gb_hat   <- approxfun(dEPA_CV)

# Estimated cummulative density function
Gb_hat   <- CDF(dEPA_CV) 

# Estimated valuations (eq. 6 in GPV)
val_hat  <- function(BIDS, NUMBER_OF_BIDDERS) {
  results <- BIDS + (1/(NUMBER_OF_BIDDERS-1))*(Gb_hat(BIDS)/gb_hat(BIDS))
  return(results)
}

vHat <- val_hat(dt$bids, nBidders) # Evaluates bids into GPV's formula to get the estimated valuations

# Note to myself: work with that CDF

#yGb <- c(cumsum(dEPA_CV$y))
#xGb <- c(dEPA_CV$x)
#Gb  <- data.frame(yGb,xGb)
#Gb_hat <- approxfun(Gb)

```

## Question 5

The plot of the estimated density can be found in Question 6. The code is attached below.

```{r q5 density of valuations}

# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
fv <- density(vHat, bw.nrd0(vHat), adjust = 1, 
                kernel = c("epanechnikov"),
                weights = NULL, n=nBids)

```

## Question 6 (also answering Q6 from the "followup.md" file)

```{r q6 histogram and fit for valuations}

# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])

# Fitting a gamma distribution - 
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])

# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1, 
                kernel = c("epanechnikov"),
                weights = NULL)

plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best

lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=6, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "limegreen", lwd = 4)

legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
       col=c("violet", "deepskyblue2", "limegreen"), lty = c(1,6,1), lwd = c(5,3,4), cex=.75)

# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal    <- fitLN_val$estimate[1] # Location parameter
sdVal      <- fitLN_val$estimate[2]  # Scale parameter

```

Given the plot above, it seems to be that the log normal distribution is the best fit for the estimated distribution of valuations. The Epanechnikov kernel density and gamma density do a great job too, but they do not capture the high frequency of valuations before and after the value of 2.

In regards to the estimated location and scale parameter, from the log normal density, these are `r round(meanVal, 2)` and `r round(sdVal, 2)`, respectively. [Recall that these are values in logs, so if -for instance- we want to compare the mean with the actual value of any valuation we should use the exponential function; in that way, the mean of the estimated valuations is exp(mean(e.Valuation))=`r round(exp(meanVal), 2)`]. Since we are simulating bids from a first price sealed bid, we should expect the mean of valuations to be higher than the mean of bids (`r round(mean(dt$bids), 2)`), as the theory predicts; so we are in the right track!

## Question 7 (from the "followup.md" file)

Comment: I did not understand what this part "... N=100 auction pairs" was refering to, so I did it in a different way. I think what follows is intuitive. As a summary, I drew values for each bidder and then I calculated the winning bid for each auction. I did this M=1,000 times for each case: 3 and 4 bidders. Finally, I took the average of the difference between the two cases in order to get the expected difference in the winning bid. 

```{r q7 simulation}

# Step 1: Approximate the recovered distribution of values using a log normal
fitLN_val <- fitdistr(vHat, "lognormal") 

#Step 2: Use these estimated log normal parameters to recompute the bid function for auctions with 3 and 4 bidders (It is a function called "sBid")

F_lN     <- function(INPUT){plnorm(INPUT, fitLN_val$estimate[1], fitLN_val$estimate[2])} 

sBid     <- function(VALUATIONS, NUMBER_OF_BIDDERS){
  
  rVal   <- rlnorm(seq(0,5,length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])  
  sVal   <- sort(sample(rVal, NUMBER_OF_BIDDERS, replace = TRUE, prob = NULL)) # Sample valuations of NUMBER_OF_BIDDERS bidders
 
  eqBids <- vector("numeric", NUMBER_OF_BIDDERS) ## Initializing vector of equilibrium bids (eq. 1 in GPV)
   for (i in 1:NUMBER_OF_BIDDERS){
  eqBids[i] <- sVal[i]-(1/(F_lN(sVal[i])^(NUMBER_OF_BIDDERS-1)))*
    integrate(function(INPUT) F_lN(INPUT)^NUMBER_OF_BIDDERS,min(sVal),sVal[i])$value
  }
  
  wBid   <- max(eqBids) # Returns the winning bid
  return(wBid)
}

# Step 3: Simulate 1,000 times the equilibrium bids for auctions with 3 and 4 bidders, respectively. 

M       <- 1000 # # of simulations
wBid3   <- vector("numeric", M) # Initializing vector of winning bids for auctions with 3 bidders
wBid4   <- vector("numeric", M) # Initializing vector of winning bids for auctions with 4 bidders

for (i in 1:M){
  wBid3[i] <- sBid(vHat,3)  
  wBid4[i] <- sBid(vHat,4)  
}
  
# Step 4: Computes the expected difference in the winning bid going from 3 to 4 bidders

 theta      <- wBid4-wBid3
 mean_theta <- mean(theta)

# Step 5: Confidence interval (I am assuming a normal distribution) 

 error    <- qnorm(0.95)*sd(theta)/sqrt(M)
 CI_theta <- c(mean_theta-error, mean_theta+error)

```

The expected difference in the winning bid ($E(\theta)$) going from 3 to 4 bidders is  `r round(mean_theta, 2)`, and its confidence interval is [`r round(CI_theta[1], 2)`, `r round(CI_theta[2], 2)`].



