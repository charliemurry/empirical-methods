lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=3, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "green", lty=3, lwd = 3)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "green"), lty = c(1,6,3,1), lwd = c(3,3,3,3), cex=.75)
# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal <- mean(exp(dLNORM_val)) # Location parameter
sdVal <- sd(exp(dLNORM_val))  # Scale parameter
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=3, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "darkgreen", lty=3, lwd = 3)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "darkgreen"), lty = c(1,6,3,1), lwd = c(3,3,3,3), cex=.75)
# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal    <- fitLN_val$estimate[1] # Location parameter
sdVal      <- fitLN_val$estimate[2]  # Scale parameter
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=3, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "darkred", lty=3, lwd = 3)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "darkred"), lty = c(1,6,3,1), lwd = c(3,3,3,3), cex=.75)
# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal    <- fitLN_val$estimate[1] # Location parameter
sdVal      <- fitLN_val$estimate[2]  # Scale parameter
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=3, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "darkred", lty=3, lwd = 3)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "darkred"), lty = c(1,6,3,1), lwd = c(3,3,3,3), cex=.75)
# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal    <- fitLN_val$estimate[1] # Location parameter
sdVal      <- fitLN_val$estimate[2]  # Scale parameter
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=6, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "darkred", lwd = 3)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "darkred"), lty = c(1,6,3) lwd = c(5,3,3), cex=.75)
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=6, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "darkred", lwd = 3)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "darkred"), lty = c(1,6,3) lwd = c(5,3,3), cex=.75)
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=6, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "darkred", lwd = 3)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "darkred"), lty = c(1,6,3), lwd = c(5,3,3), cex=.75)
# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal    <- fitLN_val$estimate[1] # Location parameter
sdVal      <- fitLN_val$estimate[2]  # Scale parameter
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=6, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "darkred", lwd = 3)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "darkred"), lty = c(1,6,3), lwd = c(5,3,3), cex=.75)
# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal    <- fitLN_val$estimate[1] # Location parameter
sdVal      <- fitLN_val$estimate[2]  # Scale parameter
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=6, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "limegreen", lwd = 3)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "limegreen"), lty = c(1,6,3), lwd = c(5,3,3), cex=.75)
# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal    <- fitLN_val$estimate[1] # Location parameter
sdVal      <- fitLN_val$estimate[2]  # Scale parameter
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=6, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "limegreen", lwd = 3)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "limegreen"), lty = c(1,6,3), lwd = c(5,3,3), cex=.75)
# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal    <- fitLN_val$estimate[1] # Location parameter
sdVal      <- fitLN_val$estimate[2]  # Scale parameter
fitLN_val$estimate[1]
exp(fitLN_val$estimate[1])
exp(fitLN_val$estimate[1])>mean(dt$bids)
knitr::opts_chunk$set(echo = T)
rm(list=ls()) # clean all objects
#getwd() # give us the directory in which we are working in
library(MASS)
library(latex2exp)
library(spatstat)
setwd("/Users/gsalazar/Documents/GitHub/empirical-methods/ProblemSets/pset1/")
dt <- read.csv(file = 'bids1.csv', header=FALSE, dec = ".")
names(dt)[1]<-paste("bids")
nBids <- length(dt$bids) # number of bids to draw from an estimated density function
ht    <- hist(dt$bids, main = "Histogram of Bids", xlab = "Bids", col = "darkolivegreen2", breaks = 14) # histogram of bids - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
# Fitting a normal distribution
fitN  <- fitdistr(dt$bids, "normal")
dNORM <- dnorm(seq(0,5,length=nBids), fitN$estimate[1], fitN$estimate[2])
# Using a Gaussian Kernel - note that nrd0 uses the Silverman's ROT
dGAUS <- density(dt$bids, bw.nrd0(dt$bids), adjust = 1,
kernel = c("gaussian"),
weights = NULL)
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA  <- density(dt$bids, bw.nrd0(dt$bids), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
# Least-squares cross-validation for bandwidth - LOOCV
FUN   <- function(optBw){
fhat=Vectorize(function(x) density(dt$bids,from=x,to=x,n=1,bw=optBw)$y)
fhati=Vectorize(function(i) density(dt$bids[-i], from=dt$bids[i],to=dt$bids[i],n=1,bw=optBw)$y)
F=fhati(1:length(dt$bids))
return(integrate(function(x) fhat(x)^2,-Inf,Inf)$value-2*mean(F))
}
vx   <- seq(.05,0.6,by=.01)
vy   <- Vectorize(FUN)(vx)
plot(vx,vy, col = "darkgreen", lwd = 2)
hOpt <- optimize(FUN,interval=c(.05,6))
hCV  <- hOpt$minimum
dEPA_CV <- density(dt$bids, bw=hCV, adjust = 1,
kernel = c("epanechnikov"),
weights = NULL, n=nBids)
plot(ht, freq = FALSE, xlim=c(0,5) ,ylim=c(0,.6), col = "darkolivegreen2", xlab = "Bid Values", main = "Histogram of bids + Estimated densities")
lines(seq(0,5,length = nBids),dNORM, col = "darkorchid1", lwd = 3)
lines(dGAUS, col = "deepskyblue", lwd = 4)
lines(dEPA, col = "green4", lty=1, lwd = 3)
lines(dEPA_CV, col = "maroon", lty=6, lwd = 3)
legend(3.5, .55, legend=c("Normal fit","Gaussian (ROT)", "Epanechnikov (ROT)", "Epanechnikov (LOOCV)"),
col=c("darkorchid1","deepskyblue", "green4", "maroon"), lty = c(1,1,1,6), lwd = c(3,4,3,3), cex=.75)
nBidders <- 3 # number of bidders
# Estimated density function
gb_hat   <- approxfun(dEPA_CV)
# Estimated cummulative density function
Gb_hat   <- CDF(dEPA_CV)
# Estimated valuations (using GPV)
val_hat  <- function(BIDS, NUMBER_OF_BIDDERS) {
results <- BIDS + (1/(NUMBER_OF_BIDDERS-1))*(Gb_hat(BIDS)/gb_hat(BIDS))
return(results)
}
vHat <- val_hat(dt$bids, nBidders) # Evaluates bids into GPV's formula to get the estimated valuations
# Note to myself: work with that CDF
#yGb <- c(cumsum(dEPA_CV$y))
#xGb <- c(dEPA_CV$x)
#Gb  <- data.frame(yGb,xGb)
#Gb_hat <- approxfun(Gb)
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
fv <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL, n=nBids)
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=6, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "limegreen", lwd = 4)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "limegreen"), lty = c(1,6,1), lwd = c(5,3,4), cex=.75)
# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal    <- fitLN_val$estimate[1] # Location parameter
sdVal      <- fitLN_val$estimate[2]  # Scale parameter
# Step 1: Approximate the recovered distribution of values using a log normal
fitLN_val <- fitdistr(vHat, "lognormal")
#Step 2: Use these estimated log normal parameters to recompute the bid function for auctions with 3 and 4 bidders (It is a function called "sBid")
F_lN     <- function(INPUT){plnorm(INPUT, fitLN_val$estimate[1], fitLN_val$estimate[2])}
sBid     <- function(VALUATIONS, NUMBER_OF_BIDDERS){
rVal   <- rlnorm(seq(0,5,length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
sVal   <- sort(sample(rVal, NUMBER_OF_BIDDERS, replace = TRUE, prob = NULL)) # Sample valuations of NUMBER_OF_BIDDERS bidders
eqBids <- vector("numeric", NUMBER_OF_BIDDERS) ## Initializing vector of equilibrium bids
for (i in 1:NUMBER_OF_BIDDERS){
eqBids[i] <- sVal[i]-(1/(F_lN(sVal[i])^(NUMBER_OF_BIDDERS-1)))*
integrate(function(INPUT) F_lN(INPUT)^NUMBER_OF_BIDDERS,min(sVal),sVal[i])$value
}
wBid   <- max(eqBids) # Returns the winning bid
return(wBid)
}
# Step 3: Simulate 1,000 times the equilibrium bids for auctions with 3 and 4 bidders, respectively.
M       <- 1000 # # of simulations
wBid3   <- vector("numeric", M) # Initializing vector of winning bids for auctions with 3 bidders
wBid4   <- vector("numeric", M) # Initializing vector of winning bids for auctions with 4 bidders
for (i in 1:M){
wBid3[i] <- sBid(vHat,3)
wBid4[i] <- sBid(vHat,4)
}
# Step 4: Computes the expected difference in the winning bid going from 3 to 4 bidders
theta      <- wBid4-wBid3
mean_theta <- mean(theta)
# Step 5: Confidence interval (I am assuming a normal distribution)
error    <- qnorm(0.95)*sd(theta)/sqrt(M)
CI_theta <- c(mean_theta-error, mean_theta+error)
knitr::opts_chunk$set(echo = T)
rm(list=ls()) # clean all objects
#getwd() # give us the directory in which we are working in
library(MASS)
library(latex2exp)
library(spatstat)
setwd("/Users/gsalazar/Documents/GitHub/empirical-methods/ProblemSets/pset1/")
dt <- read.csv(file = 'bids1.csv', header=FALSE, dec = ".")
names(dt)[1]<-paste("bids")
nBids <- length(dt$bids) # number of bids to draw from an estimated density function
ht    <- hist(dt$bids, main = "Histogram of Bids", xlab = "Bids", col = "darkolivegreen2", breaks = 14) # histogram of bids - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
# Fitting a normal distribution
fitN  <- fitdistr(dt$bids, "normal")
dNORM <- dnorm(seq(0,5,length=nBids), fitN$estimate[1], fitN$estimate[2])
# Using a Gaussian Kernel - note that nrd0 uses the Silverman's ROT
dGAUS <- density(dt$bids, bw.nrd0(dt$bids), adjust = 1,
kernel = c("gaussian"),
weights = NULL)
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA  <- density(dt$bids, bw.nrd0(dt$bids), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
# Least-squares cross-validation for bandwidth - LOOCV
FUN   <- function(optBw){
fhat=Vectorize(function(x) density(dt$bids,from=x,to=x,n=1,bw=optBw)$y)
fhati=Vectorize(function(i) density(dt$bids[-i], from=dt$bids[i],to=dt$bids[i],n=1,bw=optBw)$y)
F=fhati(1:length(dt$bids))
return(integrate(function(x) fhat(x)^2,-Inf,Inf)$value-2*mean(F))
}
vx   <- seq(.05,0.6,by=.01)
vy   <- Vectorize(FUN)(vx)
plot(vx,vy, col = "darkgreen", lwd = 2)
hOpt <- optimize(FUN,interval=c(.05,6))
hCV  <- hOpt$minimum
dEPA_CV <- density(dt$bids, bw=hCV, adjust = 1,
kernel = c("epanechnikov"),
weights = NULL, n=nBids)
plot(ht, freq = FALSE, xlim=c(0,5) ,ylim=c(0,.6), col = "darkolivegreen2", xlab = "Bid Values", main = "Histogram of bids + Estimated densities")
lines(seq(0,5,length = nBids),dNORM, col = "darkorchid1", lwd = 3)
lines(dGAUS, col = "deepskyblue", lwd = 4)
lines(dEPA, col = "green4", lty=1, lwd = 3)
lines(dEPA_CV, col = "maroon", lty=6, lwd = 3)
legend(3.5, .55, legend=c("Normal fit","Gaussian (ROT)", "Epanechnikov (ROT)", "Epanechnikov (LOOCV)"),
col=c("darkorchid1","deepskyblue", "green4", "maroon"), lty = c(1,1,1,6), lwd = c(3,4,3,3), cex=.75)
nBidders <- 3 # number of bidders
# Estimated density function
gb_hat   <- approxfun(dEPA_CV)
# Estimated cummulative density function
Gb_hat   <- CDF(dEPA_CV)
# Estimated valuations (eq. 6 in GPV)
val_hat  <- function(BIDS, NUMBER_OF_BIDDERS) {
results <- BIDS + (1/(NUMBER_OF_BIDDERS-1))*(Gb_hat(BIDS)/gb_hat(BIDS))
return(results)
}
vHat <- val_hat(dt$bids, nBidders) # Evaluates bids into GPV's formula to get the estimated valuations
# Note to myself: work with that CDF
#yGb <- c(cumsum(dEPA_CV$y))
#xGb <- c(dEPA_CV$x)
#Gb  <- data.frame(yGb,xGb)
#Gb_hat <- approxfun(Gb)
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
fv <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL, n=nBids)
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=6, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "limegreen", lwd = 4)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "limegreen"), lty = c(1,6,1), lwd = c(5,3,4), cex=.75)
# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal    <- fitLN_val$estimate[1] # Location parameter
sdVal      <- fitLN_val$estimate[2]  # Scale parameter
# Step 1: Approximate the recovered distribution of values using a log normal
fitLN_val <- fitdistr(vHat, "lognormal")
#Step 2: Use these estimated log normal parameters to recompute the bid function for auctions with 3 and 4 bidders (It is a function called "sBid")
F_lN     <- function(INPUT){plnorm(INPUT, fitLN_val$estimate[1], fitLN_val$estimate[2])}
sBid     <- function(VALUATIONS, NUMBER_OF_BIDDERS){
rVal   <- rlnorm(seq(0,5,length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
sVal   <- sort(sample(rVal, NUMBER_OF_BIDDERS, replace = TRUE, prob = NULL)) # Sample valuations of NUMBER_OF_BIDDERS bidders
eqBids <- vector("numeric", NUMBER_OF_BIDDERS) ## Initializing vector of equilibrium bids (eq. 1 in GPV)
for (i in 1:NUMBER_OF_BIDDERS){
eqBids[i] <- sVal[i]-(1/(F_lN(sVal[i])^(NUMBER_OF_BIDDERS-1)))*
integrate(function(INPUT) F_lN(INPUT)^NUMBER_OF_BIDDERS,min(sVal),sVal[i])$value
}
wBid   <- max(eqBids) # Returns the winning bid
return(wBid)
}
# Step 3: Simulate 1,000 times the equilibrium bids for auctions with 3 and 4 bidders, respectively.
M       <- 1000 # # of simulations
wBid3   <- vector("numeric", M) # Initializing vector of winning bids for auctions with 3 bidders
wBid4   <- vector("numeric", M) # Initializing vector of winning bids for auctions with 4 bidders
for (i in 1:M){
wBid3[i] <- sBid(vHat,3)
wBid4[i] <- sBid(vHat,4)
}
# Step 4: Computes the expected difference in the winning bid going from 3 to 4 bidders
theta      <- wBid4-wBid3
mean_theta <- mean(theta)
# Step 5: Confidence interval (I am assuming a normal distribution)
error    <- qnorm(0.95)*sd(theta)/sqrt(M)
CI_theta <- c(mean_theta-error, mean_theta+error)
knitr::opts_chunk$set(echo = T)
rm(list=ls()) # clean all objects
#getwd() # give us the directory in which we are working in
library(MASS)
library(latex2exp)
library(spatstat)
setwd("/Users/gsalazar/Documents/GitHub/empirical-methods/ProblemSets/pset1/")
dt <- read.csv(file = 'bids1.csv', header=FALSE, dec = ".")
names(dt)[1]<-paste("bids")
nBids <- length(dt$bids) # number of bids to draw from an estimated density function
ht    <- hist(dt$bids, main = "Histogram of Bids", xlab = "Bids", col = "darkolivegreen2", breaks = 14) # histogram of bids - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
# Fitting a normal distribution
fitN  <- fitdistr(dt$bids, "normal")
dNORM <- dnorm(seq(0,5,length=nBids), fitN$estimate[1], fitN$estimate[2])
# Using a Gaussian Kernel - note that nrd0 uses the Silverman's ROT
dGAUS <- density(dt$bids, bw.nrd0(dt$bids), adjust = 1,
kernel = c("gaussian"),
weights = NULL)
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA  <- density(dt$bids, bw.nrd0(dt$bids), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
# Least-squares cross-validation for bandwidth - LOOCV
FUN   <- function(optBw){
fhat=Vectorize(function(x) density(dt$bids,from=x,to=x,n=1,bw=optBw)$y)
fhati=Vectorize(function(i) density(dt$bids[-i], from=dt$bids[i],to=dt$bids[i],n=1,bw=optBw)$y)
F=fhati(1:length(dt$bids))
return(integrate(function(x) fhat(x)^2,-Inf,Inf)$value-2*mean(F))
}
vx   <- seq(.05,0.6,by=.01)
vy   <- Vectorize(FUN)(vx)
plot(vx,vy, col = "darkgreen", lwd = 2)
hOpt <- optimize(FUN,interval=c(.05,6))
hCV  <- hOpt$minimum
dEPA_CV <- density(dt$bids, bw=hCV, adjust = 1,
kernel = c("epanechnikov"),
weights = NULL, n=nBids)
plot(ht, freq = FALSE, xlim=c(0,5) ,ylim=c(0,.6), col = "darkolivegreen2", xlab = "Bid Values", main = "Histogram of bids + Estimated densities")
lines(seq(0,5,length = nBids),dNORM, col = "darkorchid1", lwd = 3)
lines(dGAUS, col = "deepskyblue", lwd = 4)
lines(dEPA, col = "green4", lty=1, lwd = 3)
lines(dEPA_CV, col = "maroon", lty=6, lwd = 3)
legend(3.5, .55, legend=c("Normal fit","Gaussian (ROT)", "Epanechnikov (ROT)", "Epanechnikov (LOOCV)"),
col=c("darkorchid1","deepskyblue", "green4", "maroon"), lty = c(1,1,1,6), lwd = c(3,4,3,3), cex=.75)
nBidders <- 3 # number of bidders
# Estimated density function
gb_hat   <- approxfun(dEPA_CV)
# Estimated cummulative density function
Gb_hat   <- CDF(dEPA_CV)
# Estimated valuations (eq. 6 in GPV)
val_hat  <- function(BIDS, NUMBER_OF_BIDDERS) {
results <- BIDS + (1/(NUMBER_OF_BIDDERS-1))*(Gb_hat(BIDS)/gb_hat(BIDS))
return(results)
}
vHat <- val_hat(dt$bids, nBidders) # Evaluates bids into GPV's formula to get the estimated valuations
# Note to myself: work with that CDF
#yGb <- c(cumsum(dEPA_CV$y))
#xGb <- c(dEPA_CV$x)
#Gb  <- data.frame(yGb,xGb)
#Gb_hat <- approxfun(Gb)
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
fv <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL, n=nBids)
# Fitting a normal distribution
fitLN_val  <- fitdistr(vHat, "lognormal")
dLNORM_val <- dlnorm(seq(min(vHat),max(vHat),length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
# Fitting a gamma distribution -
fitG_val   <- fitdistr(vHat, "gamma")
dGAMMA_val <- dgamma(seq(min(vHat),max(vHat),length=nBids), fitG_val$estimate[1], fitG_val$estimate[2])
# Using an Epanechnikov Kernel - note that nrd0 uses the Silverman's ROT
dEPA_val   <- density(vHat, bw.nrd0(vHat), adjust = 1,
kernel = c("epanechnikov"),
weights = NULL)
plot(hist(vHat, breaks = 20, main = "Histogram of Estimated Valuations", xlab=TeX('$\\hat{v}$')), freq = FALSE, ylim=c(0,.4), xlim=c(0,10), col = "khaki1", xlab =TeX('Estimated Valuations ($\\hat{v}$)'), main = "Histogram of Est. Valuations + Estimated densities") # histogram of valuations - note the # of breaks was augmented in order to recognize graphically which empirical density fits best
lines(dEPA_val, col = "violet", lwd = 5)
lines(seq(min(vHat),max(vHat),length = nBids), dGAMMA_val, col = "deepskyblue2", lty=6, lwd = 3)
lines(seq(min(vHat),max(vHat),length = nBids), dLNORM_val, col = "limegreen", lwd = 4)
legend(7, .35, legend=c("Epanechnikov (ROT)", "Gamma fit", "Lognormal fit"),
col=c("violet", "deepskyblue2", "limegreen"), lty = c(1,6,1), lwd = c(5,3,4), cex=.75)
# Given the analysis below, we just take the mean and the standard deviation from the Gaussian kernel density of valuations
meanVal    <- fitLN_val$estimate[1] # Location parameter
sdVal      <- fitLN_val$estimate[2]  # Scale parameter
# Step 1: Approximate the recovered distribution of values using a log normal
fitLN_val <- fitdistr(vHat, "lognormal")
#Step 2: Use these estimated log normal parameters to recompute the bid function for auctions with 3 and 4 bidders (It is a function called "sBid")
F_lN     <- function(INPUT){plnorm(INPUT, fitLN_val$estimate[1], fitLN_val$estimate[2])}
sBid     <- function(VALUATIONS, NUMBER_OF_BIDDERS){
rVal   <- rlnorm(seq(0,5,length=nBids), fitLN_val$estimate[1], fitLN_val$estimate[2])
sVal   <- sort(sample(rVal, NUMBER_OF_BIDDERS, replace = TRUE, prob = NULL)) # Sample valuations of NUMBER_OF_BIDDERS bidders
eqBids <- vector("numeric", NUMBER_OF_BIDDERS) ## Initializing vector of equilibrium bids (eq. 1 in GPV)
for (i in 1:NUMBER_OF_BIDDERS){
eqBids[i] <- sVal[i]-(1/(F_lN(sVal[i])^(NUMBER_OF_BIDDERS-1)))*
integrate(function(INPUT) F_lN(INPUT)^NUMBER_OF_BIDDERS,min(sVal),sVal[i])$value
}
wBid   <- max(eqBids) # Returns the winning bid
return(wBid)
}
# Step 3: Simulate 1,000 times the equilibrium bids for auctions with 3 and 4 bidders, respectively.
M       <- 1000 # # of simulations
wBid3   <- vector("numeric", M) # Initializing vector of winning bids for auctions with 3 bidders
wBid4   <- vector("numeric", M) # Initializing vector of winning bids for auctions with 4 bidders
for (i in 1:M){
wBid3[i] <- sBid(vHat,3)
wBid4[i] <- sBid(vHat,4)
}
# Step 4: Computes the expected difference in the winning bid going from 3 to 4 bidders
theta      <- wBid4-wBid3
mean_theta <- mean(theta)
# Step 5: Confidence interval (I am assuming a normal distribution)
error    <- qnorm(0.95)*sd(theta)/sqrt(M)
CI_theta <- c(mean_theta-error, mean_theta+error)
