{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Finding (and Fixed Points)\n",
    "\n",
    "There are many many examples from economics we seek to either find the root or the fixed point to a non-linear of (often many) equations, which cannot be computed analytically.\n",
    "\n",
    "Many estimation algorithms for equilibrium problems involve a nested structure where there is some root-finding problem in the inner nest: BLP, dynamic discrete choice (rust, labor models), trade models, steady state of dynamic models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Defn:* Roots.\n",
    "A function $g$ from $R^n$ to $R^n$ is given and one must find a vector $x$ that satisfies $x = g(x)$.\n",
    "\n",
    "<img src=\"files/graph1.png\" width=\"75%\"/>\n",
    "\n",
    "\n",
    "#### Examples\n",
    "\n",
    "- Demand and Supply market clearing.\n",
    "- FOCs from an optimization problem.\n",
    "\n",
    "\n",
    "### Iterative Methods\n",
    "\n",
    "We will consider methods that *systematically* look over the range of $x$ until $f(x) = 0$\n",
    "\n",
    "### 1. Bisection Method\n",
    "\n",
    "Intermediate Value Theorem: If a continuous real-valued function assumes two distinct values, then it must assume all values in between. \n",
    "\n",
    "If $f$ is continuous and $f(a)$ and $f(b)$ have different signs, then there must be at least one root $x$ in $[a,b]$.\n",
    "\n",
    "Evaluate $f$ at the bisection of $a$ and $b$. Take the new interval to be the bisected interval with endpoints of different signs. Repeat. \n",
    "\n",
    "<img src=\"files/graph1.png\" width=\"75%\"/>\n",
    "\n",
    "##### Bisection Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up packages\n",
    "using Printf\n",
    "# import Pkg\n",
    "# Pkg.add(\"Plots\")\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = 3 + 50*x.^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -6.0;\n",
    "b = 12.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-4;\n",
    "s = sign(f(a)); # sign if the left boundary\n",
    "x = (a+b)/2; # inital midpoint\n",
    "d = (b-a)/2;\n",
    "xsave=[];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution using user written code: x = -0.39155\n",
      "\n",
      "Are we at a zero? f(x) = -0.00141"
     ]
    }
   ],
   "source": [
    "while d>tol\n",
    "    d=d/2; # length to cut the next interval\n",
    "    push!(xsave, x)\n",
    "    \n",
    "    if s == sign(f(x))\n",
    "        x = x+d;\n",
    "    else\n",
    "        x = x-d;\n",
    "    end\n",
    "end\n",
    "push!(xsave,x)\n",
    "\n",
    "@printf \"Solution using user written code: x = %3.5f\\n\\n\" x\n",
    "\n",
    "@printf \"Are we at a zero? f(x) = %3.5f\" f(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros and cons of bisection**\n",
    "\n",
    "* Pro: Guaranteed to find a root.\n",
    "* Con: Slow (no gradient information).\n",
    "* Con: Will only find one root.\n",
    "* Con: Only good for single variable functions\n",
    "* Con: Can be very slow b/c it does not use info on shape of function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Function iteration\n",
    "\n",
    "* Supply a guess $x^0$\n",
    "* Use the updating rule $x^{(t+1)} \\leftarrow g(x^{(t)})$.\n",
    " \n",
    "The starting guess must be close to the fixed point where $||g'(x*)||<1$\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "<img src=\"files/graph2.png\" width=\"75%\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fncIteration (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function fncIteration(g::Function,x::Float64)\n",
    "\n",
    "    d = 100.0\n",
    "    tol = 1e-4\n",
    "    \n",
    "    while d>tol\n",
    "    \n",
    "        d = abs(x-g(x))\n",
    "        x = g(x)\n",
    "    \n",
    "    end\n",
    "\n",
    "    return x\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution using user written code: x = 0.99993"
     ]
    }
   ],
   "source": [
    "h(x) = x.^0.5;\n",
    "x = 0.1\n",
    "\n",
    "sol = fncIteration(h,x)\n",
    "\n",
    "@printf \"Solution using user written code: x = %3.5f\" sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Newton's Method\n",
    "\n",
    "* Use derivative information\n",
    "* Probably most common method.\n",
    "* Sometimes we know the derivative (pen a paper).\n",
    "* Sometimes we need to approximate the derivative.\n",
    "* Same thing goes with second derivatives. \n",
    "\n",
    "*The idea:*\n",
    "\n",
    "1. guess a point\n",
    "2. linearize the function around that point \n",
    "3. find the root of the linear function using taylor expansion\n",
    "4. use that point as your new guess and repeat\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "<img src=\"files/graph3.png\" width=\"75%\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "First-order Taylor approximation: $f(x)\\approx f(x^t) + f'(x^t)(x - x^t) = 0$\n",
    "\n",
    "which yields the following iteration rule: $x^{t+1}\\leftarrow x^t - [f'(x^t)]^{-1}f(x^t)$\n",
    "\n",
    "* *What do you notice about this iterative method?*\n",
    "* We need to know the derivative!\n",
    "* We will discuss this in detail later.\n",
    "\n",
    "\n",
    "*Convergence*: Judd Theorem 2.1 (page 130) -- If $x^1$ is \"sufficiently\" close\n",
    "to $x^*$, $f'(x^*)\\ne0$ and $\\mid \\frac{f''(x^*)}{f'(x^*)}<\\infty$, then the Newton\n",
    "sequence will converge to $x^*$. Also, $f$ needs to be \"smooth.\"\n",
    "\n",
    "* Warning: if $f'(x^t)$ is close to zero, then it can overshoot and cause problems\n",
    "\n",
    "\n",
    "\n",
    "#### Newton Example\n",
    "\n",
    "Simple demand function in a separate file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand\n",
    "q(p) = -12.0 + 2.0 .*p.^(-3.0);\n",
    "\n",
    "# 1st derivative of demand\n",
    "Dq(p) = -6.0 .*p.^(-4.0);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nm (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function nm(f::Function, fp::Function, x::Float64; tol=0.0000001)\n",
    "   \n",
    "    ctr=0;max_steps=100;\n",
    "     \n",
    "    while (abs(f(x)) > tol) && ctr < max_steps\n",
    "        x = x - f(x) / fp(x)\n",
    "        ctr = ctr + 1\n",
    "    end\n",
    "\n",
    "    ctr >= max_steps ? error(\"Method did not converge\") : return x\n",
    "    \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mMethod did not converge\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethod did not converge\u001b[39m",
      "",
      "Stacktrace:",
      " [1] error(::String) at ./error.jl:33",
      " [2] nm(::typeof(q), ::typeof(Dq), ::Float64; tol::Float64) at ./In[12]:10",
      " [3] nm(::Function, ::Function, ::Float64) at ./In[12]:3",
      " [4] top-level scope at In[13]:1",
      " [5] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "sol_nm = nm(q,Dq,0.1)\n",
    "\n",
    "@printf \"Solution using user written code: p = %3.5f\" sol_nm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we pick a weird starting value?\n",
    "\n",
    "Let's visualize the function first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = 0.3:0.01:0.70\n",
    "yvals = zero(similar(xvals));\n",
    "yvals = [q(xvals[i]) for i=1:length(xvals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(xvals,yvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_nm = nm(q,Dq,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_nm = nm(q,Dq,0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my simple program, it just exits with a \"NaN.\" But in a canned package, it might appear that the solver finished, so always check the full output. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi-Newton Methods\n",
    "\n",
    "Many times we do not have an analytical derivative:\n",
    "\n",
    "* It is difficult to compute analytically.\n",
    "* Potentially make mistakes in coding.\n",
    "* In general humans make mistakes.\n",
    "\n",
    "Quasi-Newton methods are the same as the Newton method, except with an approximation of the jacobian.\n",
    "\n",
    "\n",
    "#### Secant Method\n",
    "\n",
    "Univariate Newton method with Jacobian approximation.\n",
    "\n",
    "Replace $f'$ with an approximation from the last two function values:\n",
    "\n",
    "$f'(x^t) \\approx \\frac{f(x^t) - f(x^{t-1})}{x^t - x^{t-1}}$\n",
    "\n",
    "which yields the following update rule:\n",
    "\n",
    "$x^{t+1} \\leftarrow x^t - \\frac{ x^t - x^{t-1} }{ f(x^t) - f(x^{t-1}) } f(x^t)$\n",
    "\n",
    "You are constructing the approximating line through the two points $(x^t,f(x^t))$ and $(x^{t-1},f(x^{t-1}))$.\n",
    "\n",
    "\n",
    "<br><br><br><br><br> \n",
    "\n",
    "[GRAPH]\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "#### Broyden's Method\n",
    "\n",
    "Multivariate version of the secant method. \n",
    "\n",
    "* Generate a sequence of vectors $x^t$ and matrices $A^t$\n",
    "* These approximate the root and Jacobian of $f$\n",
    "* Guess $x^0$ and $A^0$.\n",
    "* $A^0$ is often set to the numerical jacobian at x^0.\n",
    "\n",
    "$f(x) \\approx f(x^t) + A^t(x-x^t) = 0$ \n",
    "which yields the following rule\n",
    "$x^{t+1} \\leftarrow x^t - (A^t)^{-1}f(x^t)$\n",
    "\n",
    "\n",
    "\n",
    "The Jacobian is also updated iteratively: \n",
    "$A^{t+1} \\leftarrow A^t + [f(x^{t+1}) - f(x^t) - A^td^t]\\frac{d^t}{d^td^t}$\n",
    "where $d^t = x^{t+1} - x^t$\n",
    "\n",
    "In practice we will update the inverse of the Jacboian to save an inversion step. \n",
    "\n",
    "**NOTE:** The sequence of approximations of the Jacobian DO NOT neccessarily  converge to the true Jacobian. \n",
    "\n",
    "\n",
    "This method will work if you start sufficiently close, and $f$ is well behaved...duh!\n",
    " \n",
    "In practice, I have used this method and it has worked very well for problems where the Jacobian diagonally dominant. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary Methods\n",
    "\n",
    "\n",
    "* Let's day we have a system of $N$ non-linear equations and $N$ unknowns. \n",
    "* $f_1(x_1,...,x_n) = 0$\n",
    "* **Also**,order the $x$'s so that the $x$ that \"affects\" $f_i$ the most is in the \"correct\" position.\n",
    "* Ex. $\\frac{\\partial f_2}{\\partial x_2}>\\frac{\\partial f_2}{\\partial x_k}\\,\\,\\, \\forall k\\ne2$\n",
    "\n",
    "* Also, consider that the Jacobian is nightmare to compute for the entire system. \n",
    "    - Hard to derive analytically\n",
    "    - Costly to compute\n",
    "- **But**, $f_i(x_i;x_{i'})$ is not hard to compute.\n",
    "\n",
    "#### Main Idea\n",
    "* Go through system of equations, one equaiton at a time. \n",
    "* Solve each equation for the \"dominant\" unknown, *conditional* on the other unknowns. \n",
    "\n",
    "#### Gauss Jacobi \n",
    "\n",
    "```\n",
    "    while not converged\n",
    "  \n",
    "      for i=1:N\n",
    "        solve f_i(x_1...,x_N)=0 for x_i only\n",
    "        \n",
    "        (actually, no need to solve f_i exactly!!)\n",
    "        \n",
    "        save updated x_i but do not use\n",
    "        \n",
    "      end\n",
    "      \n",
    "      update x vector with new guesses\n",
    "      \n",
    "      convergence: x^new == x^old\n",
    "      \n",
    "    end    \n",
    "```\n",
    "\n",
    "#### Gauss Seidel\n",
    "\n",
    "```\n",
    "    while not converged\n",
    "  \n",
    "      for i=1:N\n",
    "        solve f_i(x_1...,x_N)=0 for x_i only\n",
    "        \n",
    "        (actually, no need to solve f_i exactly!!)\n",
    "        \n",
    "        update x_i\n",
    "        \n",
    "      end\n",
    "      \n",
    "      convergence: x^new == x^old\n",
    "      \n",
    "    end\n",
    "```\n",
    "\n",
    "#### Discussion\n",
    "\n",
    "* This obviously depends on ordering.\n",
    "* Not guaranteed to work.\n",
    "* Jacobi or Seidel might work better.\n",
    "* For best results, the Jacobian needs to be \"diagonally dominant\"\n",
    "* Good to combine with other methods\n",
    "    - First GJ, switch to Newton (if possible)\n",
    "* Can use Newton/Bisection/etc at inner step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
