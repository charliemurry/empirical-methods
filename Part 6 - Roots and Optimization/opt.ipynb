{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "\n",
    "The general strategy for optimization will be the same as for root-finding: guess and check. \n",
    "\n",
    "Some approaches will use gradiant information, others will not. \n",
    "\n",
    "The typical application in empirical work is minimizing a GMM criterion or Maximum Likelihood Estimation. \n",
    "\n",
    "\n",
    "## Nelder-Mead (non-gradiant approach)\n",
    "\n",
    "Example: $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "Construct a simplex (3 points in $\\mathbb{R}^2$).\n",
    "\n",
    "Generally, we want to find the worst point in the simplex and update it -- typically by reflecting it through the opposite face.\n",
    "\n",
    "<br>\n",
    "<img src=\"files/NM1.png\" width=\"20%\"/>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $f(A)<min\\{ f(B),f(C) \\}$\n",
    "\n",
    "**First Step** reflect to point $D$.\n",
    "\n",
    "\n",
    "<img src=\"files/NM2.png\" width=\"20%\"/>\n",
    "\n",
    "\n",
    "```\n",
    "if f(D)>max{f(B),f(C)}\n",
    "    expand to $D'$\n",
    "    check f(D')\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "elseif f(D)<max{f(B),f(C)}\n",
    "    contract by halving distance between A and line(BC)\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"files/NM3.png\" width=\"20%\"/>\n",
    "\n",
    "```\n",
    "    if f(Dc)<max{f(B),f(C)}\n",
    "        shrink towards the best point\n",
    "        \n",
    "\n",
    "```\n",
    "\n",
    "<img src=\"files/NM4.png\" width=\"20%\"/>\n",
    "\n",
    "\n",
    "#### Discussion\n",
    "\n",
    "* Since no use of derivatives, this can be slow to converge.\n",
    "* Can be used for $\\mathbb{R}^n$\n",
    "* Good sometimes even if there are derivatives -- funciton might be very messy and deriavtive approximations could be misleading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: banana function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Pkg\n",
    "# Pkg.add(\"Optim\")\n",
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.649999999999999"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rosenbrock(x::Float64) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "\n",
    "a = [0.7 0.15];\n",
    "rosenbrock(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     2.261721e-08\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Nelder-Mead\n",
       "\n",
       " * Convergence measures\n",
       "    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    48\n",
       "    f(x) calls:    96\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = optimize(rosenbrock,[0.1 0.1],NelderMead())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's display the argmin we found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float64,2}:\n",
       " 1.00014  1.00028"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     2.261721e-08\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Nelder-Mead\n",
       "\n",
       " * Convergence measures\n",
       "    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    48\n",
       "    f(x) calls:    96\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Matlab the built in funciton is ```fminsearch```, but it does not have many/any options, whereas the Julia Optim function can be customized for your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Raphson Method\n",
    "\n",
    "Recall that the Newton Method for root finding used successive linear approximations to find the root. \n",
    "\n",
    "For optimization we will use successive quadratic approximations. *Hopefully* the sequence of maximums of the approximations converges to the max of the objective function. \n",
    "\n",
    "Other way to think of this -- apply Newton's root finding routine to the gradiant if the problem. \n",
    "\n",
    "##### Procedure\n",
    "\n",
    "1. Provide guess $x^0$. \n",
    "2. maximize 2nd order Taylor expansion to $f$ about $x^{(0)}$:\n",
    "$$f(x) \\approx f(x^{(0)} + f'(x^{(0)}(x - x^{(0)}) + \\frac{1}{2}(x-x^{(0)})^Tf''(x^{(0)}(x - x^{(0)})$$\n",
    "\n",
    "Iterative Rule:\n",
    "$$x^{(1)} \\leftarrow x^{(0)} - [f''(x^{(0)}]^{-1}f'(x^{(0)})$$\n",
    "\n",
    "In theory this method will converge to a _local max_ if $f$ is twice continuously\n",
    "differentiable, and if the initial guess is \"sufficiently close to a local\n",
    "max, at which $f''$ is negative definite. In practice the Hessian must be well\n",
    "conditioned, ie we do not want to divide by zero. \n",
    "\n",
    "In practice this method is not used because\n",
    "* Must compute both first and second derivatives\n",
    "* No guarantee that next step increases the function value because $f''$ need not satidfy the Second Order Conditions (negative definiteness).\n",
    "* Also, like most of the procedures we will talk about, we can only find local min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-Newton Methods\n",
    "\n",
    "In practice, we use a strategy similar to Newton-Raphson, but employ an approximation to the Hessian that we force/require to be negative-definite.\n",
    "\n",
    "This guarantees that the function can be increased in the direction of the Newton Step\n",
    "\n",
    "We will use solvers that approximate the inverse of the Hessian, and do not require any information about the true Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi-Newton update rule\n",
    "\n",
    "$$d^{(k)} = -B^{(k)}f'(x^{(k)})$$\n",
    "\n",
    "where $B^{(k)}$ is the approximation to the inverse of the Hessian.\n",
    "\n",
    "There are different apporaches to computing and updating $B$.\n",
    "\n",
    "\n",
    "**Steepest Ascent**\n",
    "\n",
    "Set $B^{(k)}=-I$.\n",
    "\n",
    "**DFP**\n",
    "\n",
    "$B \\leftarrow B + \\frac{dd^T}{d^Tu} - \\frac{Buu^TB}{u^TBu}$\n",
    "\n",
    "where $d = x^{(k+1)} - x^{(k)}$ and $u = f'(x^{(k+1)}) - f'(x^{(k)})$\n",
    "\n",
    "**BFGS**\n",
    "\n",
    "$B \\leftarrow B + \\frac{1}{d^Tu}(wd^T + dw^T - \\frac{w^Tu}{d^Tu}dd^T)$\n",
    "\n",
    "where $w = d - Bu$\n",
    "\n",
    "**L-BFGS**\n",
    "\n",
    "Same rule as BFGS but only store certain vectors in the inverse Hessian and uses a history of the apporximations. This can work well in very large problems where a dense Hessian can take up a lot of memory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to Nelder Mead\n",
    "\n",
    "QN should be faster than Nelder Mead. So let's try it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000086 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000076 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000065 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000063 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000086 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000040 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000044 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000057 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000047 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000035 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000604 seconds (308 allocations: 8.891 KiB)\n",
      "  0.001044 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000077 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000064 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000041 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000050 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000048 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000047 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000075 seconds (308 allocations: 8.891 KiB)\n",
      "  0.000061 seconds (308 allocations: 8.891 KiB)\n"
     ]
    }
   ],
   "source": [
    "for ix = 1:20\n",
    "    @time optimize(rosenbrock,[0.1 0.1],NelderMead())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.195620 seconds (43.91 k allocations: 2.040 MiB)\n",
      "  0.000213 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000140 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000149 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000689 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000177 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000174 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000154 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000152 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000143 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000122 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000136 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000156 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000149 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000699 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000263 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000681 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000180 seconds (762 allocations: 31.812 KiB)\n",
      "  0.001214 seconds (762 allocations: 31.812 KiB)\n",
      "  0.000315 seconds (762 allocations: 31.812 KiB)\n"
     ]
    }
   ],
   "source": [
    "for ix = 1:20\n",
    "    @time optimize(rosenbrock,[0.1 0.1],BFGS())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dRosen! (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's code up the derivative. But then what is going on above?!\n",
    "\n",
    "function dRosen!(G, x)\n",
    "\n",
    "    G[1] = -2.0*(1-x[1]) - 400*(x[2]-x[1]^2)*x[1]\n",
    "    G[2] = 200*(x[2]-x[1]^2)\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000082 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000048 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000044 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000043 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000041 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000042 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000042 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000041 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000041 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000081 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000082 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000208 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000050 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000047 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000044 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000042 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000043 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000042 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000085 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000046 seconds (638 allocations: 27.828 KiB)\n"
     ]
    }
   ],
   "source": [
    "for ix = 1:20\n",
    "    @time optimize(rosenbrock,dRosen!,[0.1 0.1],BFGS())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the specifc output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     5.378245e-17\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     BFGS\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 1.08e-09 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 1.08e-09 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 8.22e-18 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 1.53e-01 ≰ 0.0e+00\n",
       "    |g(x)|                 = 1.65e-12 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    19\n",
       "    f(x) calls:    57\n",
       "    ∇f(x) calls:   57\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(rosenbrock,[0.1 0.1],BFGS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     2.688832e-27\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     BFGS\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 1.08e-09 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 1.08e-09 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 8.31e-19 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 3.09e+08 ≰ 0.0e+00\n",
       "    |g(x)|                 = 1.24e-12 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    19\n",
       "    f(x) calls:    57\n",
       "    ∇f(x) calls:   57\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(rosenbrock,dRosen!,[0.1 0.1],BFGS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     2.688832e-27\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     BFGS\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 1.08e-09 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 1.08e-09 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 8.31e-19 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 3.09e+08 ≰ 0.0e+00\n",
       "    |g(x)|                 = 1.24e-12 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    19\n",
       "    f(x) calls:    57\n",
       "    ∇f(x) calls:   57\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic Differentiation (we will talk about this later)\n",
    "optimize(rosenbrock,dRosen!,[0.1 0.1],BFGS(); autodiff = :forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     2.688832e-27\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     BFGS\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 1.08e-09 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 1.08e-09 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 8.31e-19 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 3.09e+08 ≰ 0.0e+00\n",
       "    |g(x)|                 = 1.24e-12 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    19\n",
       "    f(x) calls:    57\n",
       "    ∇f(x) calls:   57\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's add the Hessian\n",
    "\n",
    "function h!(H, x)\n",
    "    H[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\n",
    "    H[1, 2] = -400.0 * x[1]\n",
    "    H[2, 1] = -400.0 * x[1]\n",
    "    H[2, 2] = 200.0\n",
    "end\n",
    "\n",
    "optimize(rosenbrock,dRosen!,h!,[0.1 0.1],BFGS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000078 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000048 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000043 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000054 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000049 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000052 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000052 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000043 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000045 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000042 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000042 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000042 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000055 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000044 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000044 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000069 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000083 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000068 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000081 seconds (638 allocations: 27.828 KiB)\n",
      "  0.000063 seconds (638 allocations: 27.828 KiB)\n"
     ]
    }
   ],
   "source": [
    "for ix = 1:20\n",
    "    @time optimize(rosenbrock,dRosen!,h!,[0.1 0.1],BFGS())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not great tests for time, but you get the point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood\n",
    "\n",
    "\n",
    "Special case where we know the form of the Hessian. Basic idea: choose a distribution funciton for some data, $y$, that depends on unknown parameters, $\\theta$. \n",
    "\n",
    "The log likelihood function is the sum of the logs of the likelihoods of each\n",
    "of the data observations: $l(\\theta, x; y) = \\sum_n ln(f(y_i;\\theta,x_i))$\n",
    "\n",
    "Define the \"score\" function as the matrix of derivatives of the LL fucntion\n",
    "evaluated at each observation: $s_i(\\theta;y) = \\frac{\\partial l(\\theta; y_i) }{\\partial \\theta}$\n",
    "\n",
    "Now consider the $n\\times k$ score matrix. The expectation of the inner product\n",
    "of the score function is equal to the negative of the expectation of the\n",
    "second derivative of the likelihood function (\"information\" matrix). This is a\n",
    "positive definite that we can use in place of the Hessian to update the search\n",
    "direction. \n",
    "\n",
    "$$d = -[s(\\theta;y)^Ts(\\theta;y)]^{-1}s(\\theta;y)^T1_n$$\n",
    "\n",
    "And the inverse \"Hessian\" is an estimate for the covariance of $\\theta$.\n",
    "\n",
    "## \"Global\" Optimization\n",
    "\n",
    "* Simulated Annealing\n",
    "* Genetic Algorithm \n",
    "* Pattern Search\n",
    "* MCMC approaches: Chernozhukov and Hong (2003)\n",
    "\n",
    "These methods can be very very slow to converge, but are useful in cases where\n",
    "you know your objective function is non-smooth or very ill-behaved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
